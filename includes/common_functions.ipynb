{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import current_timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5829bc94-843b-4c5c-92fd-8ee7bdeb285c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def add_ingestion_date(input_df):\n    output_df = input_df.withColumn(\"ingestion_date\",current_timestamp())\n    return output_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d5c51c67-c17f-4314-a394-85ffd165e606","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def overwrite_partition(dataframe, database, table, partition_key):\n    outputdf = reorder_frame_by_key(dataframe,partition_key)\n    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n    if (spark._jsparkSession.catalog().tableExists(f\"{database}.{table}\")):\n        outputdf.write.mode(\"overwrite\").insertInto(f\"{database}.{table}\")\n    else:\n        outputdf.write.mode(\"overwrite\").partitionBy(partition_key).format(\"parquet\").saveAsTable(f\"{database}.{table}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a9249ad1-fb6a-49e3-9c95-bb16b9c32ae1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# result_final_df.schema.names\ndef reorder_frame_by_key(dataframe, key):\n    columns = []\n    for column in dataframe.schema.names:\n        if column != key:\n            columns.append(column)\n    columns.append(key)\n    \n    output_df = dataframe.select(columns)\n    return output_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5961c5b4-735a-44a4-b355-0dabaf5a7880","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def df_column_to_list(input_df, column_name):\n  df_row_list = input_df.select(column_name) \\\n                        .distinct() \\\n                        .collect()\n  \n  column_value_list = [row[column_name] for row in df_row_list]\n  return column_value_list"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67060022-1452-4d1f-bafd-68da8a8206cf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):\n  spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\", \"true\")\n  from delta.tables import DeltaTable\n  if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n    deltaTable = DeltaTable.forPath(spark, f\"{folder_path}/{table_name}\")\n    deltaTable.alias(\"tgt\").merge(\n      input_df.alias(\"src\"),\n      merge_condition) \\\n      .whenMatchedUpdateAll() \\\n      .whenNotMatchedInsertAll() \\\n      .execute()\n  else:\n    input_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f87a1a0a-17a8-446e-9db7-1af0eceeb20f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9042a1ad-17d9-4f37-b43f-7dbdc563dac7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"common_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2707625431822363}},"nbformat":4,"nbformat_minor":0}
